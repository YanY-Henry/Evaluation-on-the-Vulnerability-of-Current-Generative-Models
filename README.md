# Evaluation on the Vulnerability of Current Generative Models

This is my [semester project](https://www.epfl.ch/schools/ic/education/bachelor/semester-project/) in [EPFL](https://www.epfl.ch/en/), 2024 Spring. It is a pleasure to work at [IVRL](https://www.epfl.ch/labs/ivrl/) and to be supervised by [Dr. Daichi Zhang](https://daisy-zhang.github.io) and [Prof. Sabine SÃ¼sstrunk](https://people.epfl.ch/sabine.susstrunk?lang=en).  

#### **Description:**

The rapid development of generative models has brought great changes in our daily life, such as large language models, diffusion models and even vision foundation models. However, are those models always safe enough to use? Will they cause harm to users, such as data leakage, generating biased results, or simply attacked or manipulated by attackers?  
In this project, we are interested in the safety problems of current generative models and aim to evaluate how vulnerable they are.  

#### **Key Questions:**
- Data Leakage: since the generatve models could access to large-scale training data as well as the user input data, will it cause data leakage when genereating results?
- Biased Results: will the generated results be fair enough or just a biased view of trained generative models.
- Attack by users: can we perturbate or attack target generative models to make it generating wrong or desired manipulated results?
